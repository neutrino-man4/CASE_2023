{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#import setGPU\n",
    "from importlib import reload\n",
    "import dadrah.playground.test_gradient_tape as tegrta\n",
    "import dadrah.selection.loss_strategy as ls\n",
    "import dadrah.selection.discriminator as disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.uniform([3000, 1], minval=1, maxval=100, dtype=tf.float32)\n",
    "y = tf.random.uniform([3000,1], minval=1, maxval=10, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3000, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile = 0.1\n",
    "strategy = ls.combine_loss_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 142.17120361328125\n",
      "Training loss (for one batch) at step 2: 119.99320983886719\n",
      "Training loss (for one batch) at step 4: 84.17875671386719\n",
      "Training loss (for one batch) at step 6: 54.75967025756836\n",
      "Training loss (for one batch) at step 8: 59.2042236328125\n",
      "Training loss (for one batch) at step 10: 69.57876586914062\n",
      "Training loss (for one batch) at step 12: 62.67546081542969\n",
      "Training loss (for one batch) at step 14: 54.283287048339844\n",
      "Training loss (for one batch) at step 16: 51.496402740478516\n",
      "Training loss (for one batch) at step 18: 58.47557067871094\n",
      "Training loss (for one batch) at step 20: 59.661895751953125\n",
      "Training loss (for one batch) at step 22: 58.61177062988281\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 52.76274490356445\n",
      "Training loss (for one batch) at step 2: 53.90526580810547\n",
      "Training loss (for one batch) at step 4: 49.281837463378906\n",
      "Training loss (for one batch) at step 6: 53.007118225097656\n",
      "Training loss (for one batch) at step 8: 55.670745849609375\n",
      "Training loss (for one batch) at step 10: 56.913143157958984\n",
      "Training loss (for one batch) at step 12: 52.88890838623047\n",
      "Training loss (for one batch) at step 14: 51.19029998779297\n",
      "Training loss (for one batch) at step 16: 51.64230728149414\n",
      "Training loss (for one batch) at step 18: 57.3472900390625\n",
      "Training loss (for one batch) at step 20: 56.77628707885742\n",
      "Training loss (for one batch) at step 22: 56.29369354248047\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 51.400699615478516\n",
      "Training loss (for one batch) at step 2: 52.491310119628906\n",
      "Training loss (for one batch) at step 4: 49.040306091308594\n",
      "Training loss (for one batch) at step 6: 52.43000030517578\n",
      "Training loss (for one batch) at step 8: 55.042327880859375\n",
      "Training loss (for one batch) at step 10: 55.873756408691406\n",
      "Training loss (for one batch) at step 12: 51.95214080810547\n",
      "Training loss (for one batch) at step 14: 50.696468353271484\n",
      "Training loss (for one batch) at step 16: 51.855377197265625\n",
      "Training loss (for one batch) at step 18: 57.14997100830078\n",
      "Training loss (for one batch) at step 20: 56.34794616699219\n",
      "Training loss (for one batch) at step 22: 55.72968292236328\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 51.42573165893555\n",
      "Training loss (for one batch) at step 2: 52.197147369384766\n",
      "Training loss (for one batch) at step 4: 49.058631896972656\n",
      "Training loss (for one batch) at step 6: 52.271915435791016\n",
      "Training loss (for one batch) at step 8: 54.955909729003906\n",
      "Training loss (for one batch) at step 10: 55.79328155517578\n",
      "Training loss (for one batch) at step 12: 51.772789001464844\n",
      "Training loss (for one batch) at step 14: 50.58441162109375\n",
      "Training loss (for one batch) at step 16: 51.927650451660156\n",
      "Training loss (for one batch) at step 18: 57.017486572265625\n",
      "Training loss (for one batch) at step 20: 56.18434524536133\n",
      "Training loss (for one batch) at step 22: 55.247703552246094\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 51.96382141113281\n",
      "Training loss (for one batch) at step 2: 52.0928955078125\n",
      "Training loss (for one batch) at step 4: 49.080142974853516\n",
      "Training loss (for one batch) at step 6: 51.982051849365234\n",
      "Training loss (for one batch) at step 8: 55.128517150878906\n",
      "Training loss (for one batch) at step 10: 55.85942459106445\n",
      "Training loss (for one batch) at step 12: 51.605445861816406\n",
      "Training loss (for one batch) at step 14: 50.45691680908203\n",
      "Training loss (for one batch) at step 16: 51.82404327392578\n",
      "Training loss (for one batch) at step 18: 56.85652160644531\n",
      "Training loss (for one batch) at step 20: 55.84912109375\n",
      "Training loss (for one batch) at step 22: 54.98736572265625\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 52.14524841308594\n",
      "Training loss (for one batch) at step 2: 52.10369873046875\n",
      "Training loss (for one batch) at step 4: 49.07020568847656\n",
      "Training loss (for one batch) at step 6: 51.906394958496094\n",
      "Training loss (for one batch) at step 8: 55.20832824707031\n",
      "Training loss (for one batch) at step 10: 55.900352478027344\n",
      "Training loss (for one batch) at step 12: 51.594078063964844\n",
      "Training loss (for one batch) at step 14: 50.500946044921875\n",
      "Training loss (for one batch) at step 16: 51.72510528564453\n",
      "Training loss (for one batch) at step 18: 56.77606201171875\n",
      "Training loss (for one batch) at step 20: 55.77619552612305\n",
      "Training loss (for one batch) at step 22: 54.85699462890625\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 52.38022232055664\n",
      "Training loss (for one batch) at step 2: 52.12297058105469\n",
      "Training loss (for one batch) at step 4: 49.095149993896484\n",
      "Training loss (for one batch) at step 6: 51.75818634033203\n",
      "Training loss (for one batch) at step 8: 55.37282180786133\n",
      "Training loss (for one batch) at step 10: 56.052406311035156\n",
      "Training loss (for one batch) at step 12: 51.51365661621094\n",
      "Training loss (for one batch) at step 14: 50.44202423095703\n",
      "Training loss (for one batch) at step 16: 51.55011749267578\n",
      "Training loss (for one batch) at step 18: 56.46763229370117\n",
      "Training loss (for one batch) at step 20: 55.48870086669922\n",
      "Training loss (for one batch) at step 22: 54.645301818847656\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 52.35902404785156\n",
      "Training loss (for one batch) at step 2: 52.0651741027832\n",
      "Training loss (for one batch) at step 4: 49.19624710083008\n",
      "Training loss (for one batch) at step 6: 51.64155960083008\n",
      "Training loss (for one batch) at step 8: 55.363311767578125\n",
      "Training loss (for one batch) at step 10: 55.8524169921875\n",
      "Training loss (for one batch) at step 12: 51.564476013183594\n",
      "Training loss (for one batch) at step 14: 50.667362213134766\n",
      "Training loss (for one batch) at step 16: 51.35125732421875\n",
      "Training loss (for one batch) at step 18: 56.47542953491211\n",
      "Training loss (for one batch) at step 20: 55.64720153808594\n",
      "Training loss (for one batch) at step 22: 54.76554870605469\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 52.27156066894531\n",
      "Training loss (for one batch) at step 2: 52.0621337890625\n",
      "Training loss (for one batch) at step 4: 49.204010009765625\n",
      "Training loss (for one batch) at step 6: 51.64634704589844\n",
      "Training loss (for one batch) at step 8: 55.37677001953125\n",
      "Training loss (for one batch) at step 10: 55.866798400878906\n",
      "Training loss (for one batch) at step 12: 51.5433464050293\n",
      "Training loss (for one batch) at step 14: 50.62086868286133\n",
      "Training loss (for one batch) at step 16: 51.3984375\n",
      "Training loss (for one batch) at step 18: 56.49018859863281\n",
      "Training loss (for one batch) at step 20: 55.61671447753906\n",
      "Training loss (for one batch) at step 22: 54.70806884765625\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 52.42833709716797\n",
      "Training loss (for one batch) at step 2: 52.0695686340332\n",
      "Training loss (for one batch) at step 4: 49.19348907470703\n",
      "Training loss (for one batch) at step 6: 51.62548828125\n",
      "Training loss (for one batch) at step 8: 55.41701889038086\n",
      "Training loss (for one batch) at step 10: 55.90241241455078\n",
      "Training loss (for one batch) at step 12: 51.538326263427734\n",
      "Training loss (for one batch) at step 14: 50.613441467285156\n",
      "Training loss (for one batch) at step 16: 51.38934326171875\n",
      "Training loss (for one batch) at step 18: 56.47826385498047\n",
      "Training loss (for one batch) at step 20: 55.645751953125\n",
      "Training loss (for one batch) at step 22: 54.79745101928711\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "feature_normalization (Featu (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 30)                60        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 31        \n",
      "_________________________________________________________________\n",
      "feature_un_normalization (Fe (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 2,881\n",
      "Trainable params: 2,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import dadrah.selection.quantile_regression as qure\n",
    "reload(qure)\n",
    "reload(disc)\n",
    "discriminator = disc.QRDiscriminator(quantile=quantile, loss_strategy=strategy, epochs=10, n_nodes=30)\n",
    "discriminator.fit(x, y)\n",
    "print(discriminator.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = tf.random.uniform([300, 1], minval=1, maxval=100, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(300, 1), dtype=float32, numpy=\n",
       "array([[ 9.920164 ],\n",
       "       [61.444466 ],\n",
       "       [79.72258  ],\n",
       "       [99.88019  ],\n",
       "       [11.634789 ],\n",
       "       [64.567345 ],\n",
       "       [41.09856  ],\n",
       "       [35.97979  ],\n",
       "       [65.91308  ],\n",
       "       [77.14003  ],\n",
       "       [38.83267  ],\n",
       "       [34.518124 ],\n",
       "       [40.181625 ],\n",
       "       [93.052895 ],\n",
       "       [99.196495 ],\n",
       "       [11.585918 ],\n",
       "       [92.9842   ],\n",
       "       [23.480211 ],\n",
       "       [35.9555   ],\n",
       "       [94.17641  ],\n",
       "       [24.325344 ],\n",
       "       [92.15333  ],\n",
       "       [45.68191  ],\n",
       "       [64.08234  ],\n",
       "       [56.854847 ],\n",
       "       [45.84055  ],\n",
       "       [ 8.648931 ],\n",
       "       [44.039642 ],\n",
       "       [58.54842  ],\n",
       "       [64.27069  ],\n",
       "       [51.68272  ],\n",
       "       [10.009503 ],\n",
       "       [36.914345 ],\n",
       "       [46.982475 ],\n",
       "       [65.73589  ],\n",
       "       [55.834908 ],\n",
       "       [55.98276  ],\n",
       "       [14.50957  ],\n",
       "       [96.24013  ],\n",
       "       [64.313866 ],\n",
       "       [74.743774 ],\n",
       "       [26.961506 ],\n",
       "       [27.457981 ],\n",
       "       [47.651432 ],\n",
       "       [34.95206  ],\n",
       "       [52.265972 ],\n",
       "       [14.910286 ],\n",
       "       [30.657402 ],\n",
       "       [65.82034  ],\n",
       "       [88.23235  ],\n",
       "       [45.77929  ],\n",
       "       [97.995865 ],\n",
       "       [62.954742 ],\n",
       "       [23.576384 ],\n",
       "       [29.87227  ],\n",
       "       [54.21656  ],\n",
       "       [64.8093   ],\n",
       "       [42.53352  ],\n",
       "       [96.2039   ],\n",
       "       [ 7.931669 ],\n",
       "       [61.06824  ],\n",
       "       [25.221035 ],\n",
       "       [ 9.980283 ],\n",
       "       [95.81815  ],\n",
       "       [33.242462 ],\n",
       "       [64.77581  ],\n",
       "       [19.906118 ],\n",
       "       [37.956036 ],\n",
       "       [55.990173 ],\n",
       "       [95.25363  ],\n",
       "       [51.590385 ],\n",
       "       [50.766613 ],\n",
       "       [ 2.343791 ],\n",
       "       [40.994755 ],\n",
       "       [81.74235  ],\n",
       "       [47.073288 ],\n",
       "       [26.401337 ],\n",
       "       [ 1.6518444],\n",
       "       [74.52959  ],\n",
       "       [27.232037 ],\n",
       "       [57.93472  ],\n",
       "       [23.203495 ],\n",
       "       [ 6.474877 ],\n",
       "       [93.07081  ],\n",
       "       [84.65106  ],\n",
       "       [84.95379  ],\n",
       "       [51.543106 ],\n",
       "       [27.293287 ],\n",
       "       [85.212    ],\n",
       "       [99.51857  ],\n",
       "       [36.146053 ],\n",
       "       [83.071365 ],\n",
       "       [34.51004  ],\n",
       "       [ 5.9539604],\n",
       "       [52.977215 ],\n",
       "       [70.37012  ],\n",
       "       [18.27537  ],\n",
       "       [48.27475  ],\n",
       "       [86.16727  ],\n",
       "       [74.47536  ],\n",
       "       [61.2508   ],\n",
       "       [22.854    ],\n",
       "       [79.00585  ],\n",
       "       [86.57284  ],\n",
       "       [42.553337 ],\n",
       "       [55.844315 ],\n",
       "       [55.659687 ],\n",
       "       [37.157825 ],\n",
       "       [51.415886 ],\n",
       "       [34.45575  ],\n",
       "       [80.57582  ],\n",
       "       [81.91048  ],\n",
       "       [65.740166 ],\n",
       "       [75.2      ],\n",
       "       [88.99869  ],\n",
       "       [17.302553 ],\n",
       "       [81.50557  ],\n",
       "       [89.89868  ],\n",
       "       [85.76735  ],\n",
       "       [87.771164 ],\n",
       "       [42.18793  ],\n",
       "       [93.78008  ],\n",
       "       [48.949844 ],\n",
       "       [23.436922 ],\n",
       "       [17.189293 ],\n",
       "       [31.97077  ],\n",
       "       [71.910965 ],\n",
       "       [94.61804  ],\n",
       "       [39.286007 ],\n",
       "       [33.58286  ],\n",
       "       [71.32243  ],\n",
       "       [45.050392 ],\n",
       "       [96.40006  ],\n",
       "       [28.65235  ],\n",
       "       [77.516075 ],\n",
       "       [68.17978  ],\n",
       "       [61.423637 ],\n",
       "       [64.95055  ],\n",
       "       [42.87466  ],\n",
       "       [ 6.5517883],\n",
       "       [69.35466  ],\n",
       "       [50.910156 ],\n",
       "       [41.144318 ],\n",
       "       [92.325096 ],\n",
       "       [ 2.4382756],\n",
       "       [96.245    ],\n",
       "       [82.29426  ],\n",
       "       [41.461193 ],\n",
       "       [31.903759 ],\n",
       "       [23.199802 ],\n",
       "       [74.813194 ],\n",
       "       [46.78692  ],\n",
       "       [64.20712  ],\n",
       "       [31.28025  ],\n",
       "       [83.507195 ],\n",
       "       [63.420452 ],\n",
       "       [11.142409 ],\n",
       "       [68.08517  ],\n",
       "       [72.95657  ],\n",
       "       [97.60319  ],\n",
       "       [94.02493  ],\n",
       "       [41.42869  ],\n",
       "       [26.208673 ],\n",
       "       [ 4.539678 ],\n",
       "       [17.028353 ],\n",
       "       [12.3109455],\n",
       "       [19.025309 ],\n",
       "       [32.096893 ],\n",
       "       [32.67212  ],\n",
       "       [55.340973 ],\n",
       "       [97.3783   ],\n",
       "       [82.7851   ],\n",
       "       [49.757812 ],\n",
       "       [95.16399  ],\n",
       "       [93.97331  ],\n",
       "       [94.453674 ],\n",
       "       [17.870758 ],\n",
       "       [47.26724  ],\n",
       "       [22.352474 ],\n",
       "       [12.117515 ],\n",
       "       [99.97838  ],\n",
       "       [86.66206  ],\n",
       "       [59.100243 ],\n",
       "       [ 1.3934457],\n",
       "       [63.508457 ],\n",
       "       [67.18599  ],\n",
       "       [40.004944 ],\n",
       "       [77.78187  ],\n",
       "       [ 6.3743615],\n",
       "       [38.060444 ],\n",
       "       [59.20889  ],\n",
       "       [43.58877  ],\n",
       "       [58.67892  ],\n",
       "       [84.58928  ],\n",
       "       [66.31045  ],\n",
       "       [94.95295  ],\n",
       "       [14.836266 ],\n",
       "       [87.07825  ],\n",
       "       [45.134323 ],\n",
       "       [84.78553  ],\n",
       "       [33.803257 ],\n",
       "       [58.860645 ],\n",
       "       [ 5.311074 ],\n",
       "       [68.98912  ],\n",
       "       [ 7.0655413],\n",
       "       [81.70937  ],\n",
       "       [45.67921  ],\n",
       "       [55.317074 ],\n",
       "       [ 8.80207  ],\n",
       "       [ 8.986944 ],\n",
       "       [73.87529  ],\n",
       "       [75.07908  ],\n",
       "       [26.310959 ],\n",
       "       [53.577816 ],\n",
       "       [18.14418  ],\n",
       "       [83.34254  ],\n",
       "       [71.24343  ],\n",
       "       [51.925327 ],\n",
       "       [49.79086  ],\n",
       "       [19.423653 ],\n",
       "       [82.5526   ],\n",
       "       [62.306087 ],\n",
       "       [44.859486 ],\n",
       "       [24.963758 ],\n",
       "       [47.741016 ],\n",
       "       [85.60306  ],\n",
       "       [16.986599 ],\n",
       "       [69.74888  ],\n",
       "       [44.786633 ],\n",
       "       [72.76518  ],\n",
       "       [80.66439  ],\n",
       "       [23.735918 ],\n",
       "       [87.25681  ],\n",
       "       [43.57428  ],\n",
       "       [25.36464  ],\n",
       "       [ 8.090886 ],\n",
       "       [86.56798  ],\n",
       "       [70.43952  ],\n",
       "       [ 3.3415556],\n",
       "       [29.742062 ],\n",
       "       [36.237785 ],\n",
       "       [78.82117  ],\n",
       "       [90.51795  ],\n",
       "       [17.183084 ],\n",
       "       [76.94334  ],\n",
       "       [37.07676  ],\n",
       "       [57.441902 ],\n",
       "       [53.211163 ],\n",
       "       [59.23144  ],\n",
       "       [91.630554 ],\n",
       "       [ 7.621874 ],\n",
       "       [ 1.8934138],\n",
       "       [37.05305  ],\n",
       "       [84.36232  ],\n",
       "       [23.500286 ],\n",
       "       [42.852295 ],\n",
       "       [78.61415  ],\n",
       "       [44.09882  ],\n",
       "       [90.938354 ],\n",
       "       [79.52292  ],\n",
       "       [12.154737 ],\n",
       "       [68.4149   ],\n",
       "       [96.802345 ],\n",
       "       [94.12824  ],\n",
       "       [30.894335 ],\n",
       "       [61.45353  ],\n",
       "       [26.588335 ],\n",
       "       [52.74653  ],\n",
       "       [19.725151 ],\n",
       "       [42.09287  ],\n",
       "       [ 4.849048 ],\n",
       "       [89.15388  ],\n",
       "       [55.415592 ],\n",
       "       [81.88043  ],\n",
       "       [52.362312 ],\n",
       "       [94.08648  ],\n",
       "       [86.00499  ],\n",
       "       [87.60849  ],\n",
       "       [55.5927   ],\n",
       "       [ 2.7027993],\n",
       "       [32.277767 ],\n",
       "       [96.30431  ],\n",
       "       [86.126335 ],\n",
       "       [49.470783 ],\n",
       "       [ 3.968227 ],\n",
       "       [34.236168 ],\n",
       "       [ 5.550956 ],\n",
       "       [26.500412 ],\n",
       "       [88.74038  ],\n",
       "       [74.65208  ],\n",
       "       [19.681225 ],\n",
       "       [17.843626 ],\n",
       "       [51.355885 ],\n",
       "       [78.92186  ],\n",
       "       [57.576298 ],\n",
       "       [68.70473  ],\n",
       "       [43.993923 ],\n",
       "       [32.866684 ],\n",
       "       [25.411692 ],\n",
       "       [17.25144  ]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(disc)\n",
    "y = discriminator.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.140653 ],\n",
       "       [9.301202 ],\n",
       "       [9.370041 ],\n",
       "       [9.439928 ],\n",
       "       [9.146412 ],\n",
       "       [9.3129835],\n",
       "       [9.238287 ],\n",
       "       [9.223189 ],\n",
       "       [9.318062 ],\n",
       "       [9.360418 ],\n",
       "       [9.231604 ],\n",
       "       [9.218878 ],\n",
       "       [9.235582 ],\n",
       "       [9.416879 ],\n",
       "       [9.43762  ],\n",
       "       [9.146247 ],\n",
       "       [9.416647 ],\n",
       "       [9.185789 ],\n",
       "       [9.223118 ],\n",
       "       [9.420671 ],\n",
       "       [9.188429 ],\n",
       "       [9.413841 ],\n",
       "       [9.251806 ],\n",
       "       [9.311153 ],\n",
       "       [9.284761 ],\n",
       "       [9.252274 ],\n",
       "       [9.136383 ],\n",
       "       [9.246962 ],\n",
       "       [9.290276 ],\n",
       "       [9.311865 ],\n",
       "       [9.269506 ],\n",
       "       [9.140953 ],\n",
       "       [9.225946 ],\n",
       "       [9.255642 ],\n",
       "       [9.317392 ],\n",
       "       [9.281753 ],\n",
       "       [9.282188 ],\n",
       "       [9.156066 ],\n",
       "       [9.42764  ],\n",
       "       [9.312027 ],\n",
       "       [9.3513775],\n",
       "       [9.196589 ],\n",
       "       [9.198053 ],\n",
       "       [9.257616 ],\n",
       "       [9.220158 ],\n",
       "       [9.271226 ],\n",
       "       [9.157412 ],\n",
       "       [9.207491 ],\n",
       "       [9.317711 ],\n",
       "       [9.4004135],\n",
       "       [9.252093 ],\n",
       "       [9.433567 ],\n",
       "       [9.3069   ],\n",
       "       [9.18609  ],\n",
       "       [9.205175 ],\n",
       "       [9.276979 ],\n",
       "       [9.313896 ],\n",
       "       [9.242519 ],\n",
       "       [9.427517 ],\n",
       "       [9.133975 ],\n",
       "       [9.299782 ],\n",
       "       [9.191229 ],\n",
       "       [9.140856 ],\n",
       "       [9.426214 ],\n",
       "       [9.215116 ],\n",
       "       [9.31377  ],\n",
       "       [9.17419  ],\n",
       "       [9.229018 ],\n",
       "       [9.282211 ],\n",
       "       [9.424309 ],\n",
       "       [9.269234 ],\n",
       "       [9.266804 ],\n",
       "       [9.11521  ],\n",
       "       [9.237982 ],\n",
       "       [9.377289 ],\n",
       "       [9.25591  ],\n",
       "       [9.194916 ],\n",
       "       [9.112885 ],\n",
       "       [9.35057  ],\n",
       "       [9.197388 ],\n",
       "       [9.288124 ],\n",
       "       [9.184925 ],\n",
       "       [9.129083 ],\n",
       "       [9.41694  ],\n",
       "       [9.387725 ],\n",
       "       [9.388811 ],\n",
       "       [9.269094 ],\n",
       "       [9.197569 ],\n",
       "       [9.389738 ],\n",
       "       [9.438707 ],\n",
       "       [9.2236805],\n",
       "       [9.382057 ],\n",
       "       [9.218855 ],\n",
       "       [9.127333 ],\n",
       "       [9.273324 ],\n",
       "       [9.334877 ],\n",
       "       [9.168713 ],\n",
       "       [9.259454 ],\n",
       "       [9.393166 ],\n",
       "       [9.350366 ],\n",
       "       [9.300471 ],\n",
       "       [9.183832 ],\n",
       "       [9.367458 ],\n",
       "       [9.394621 ],\n",
       "       [9.2425785],\n",
       "       [9.28178  ],\n",
       "       [9.281237 ],\n",
       "       [9.226664 ],\n",
       "       [9.268719 ],\n",
       "       [9.218695 ],\n",
       "       [9.373103 ],\n",
       "       [9.377892 ],\n",
       "       [9.317409 ],\n",
       "       [9.3531   ],\n",
       "       [9.40305  ],\n",
       "       [9.165446 ],\n",
       "       [9.376438 ],\n",
       "       [9.406147 ],\n",
       "       [9.391729 ],\n",
       "       [9.398828 ],\n",
       "       [9.241501 ],\n",
       "       [9.419334 ],\n",
       "       [9.261445 ],\n",
       "       [9.185654 ],\n",
       "       [9.165066 ],\n",
       "       [9.211365 ],\n",
       "       [9.340691 ],\n",
       "       [9.422163 ],\n",
       "       [9.232941 ],\n",
       "       [9.21612  ],\n",
       "       [9.33847  ],\n",
       "       [9.249944 ],\n",
       "       [9.428179 ],\n",
       "       [9.201576 ],\n",
       "       [9.361837 ],\n",
       "       [9.326612 ],\n",
       "       [9.301123 ],\n",
       "       [9.31443  ],\n",
       "       [9.243526 ],\n",
       "       [9.129341 ],\n",
       "       [9.331045 ],\n",
       "       [9.267227 ],\n",
       "       [9.238422 ],\n",
       "       [9.414421 ],\n",
       "       [9.115526 ],\n",
       "       [9.427655 ],\n",
       "       [9.379269 ],\n",
       "       [9.239357 ],\n",
       "       [9.211167 ],\n",
       "       [9.184913 ],\n",
       "       [9.35164  ],\n",
       "       [9.255066 ],\n",
       "       [9.311625 ],\n",
       "       [9.209328 ],\n",
       "       [9.38362  ],\n",
       "       [9.308657 ],\n",
       "       [9.144758 ],\n",
       "       [9.326256 ],\n",
       "       [9.344635 ],\n",
       "       [9.432241 ],\n",
       "       [9.42016  ],\n",
       "       [9.239262 ],\n",
       "       [9.194314 ],\n",
       "       [9.122583 ],\n",
       "       [9.164525 ],\n",
       "       [9.148683 ],\n",
       "       [9.171232 ],\n",
       "       [9.211737 ],\n",
       "       [9.213433 ],\n",
       "       [9.280296 ],\n",
       "       [9.431482 ],\n",
       "       [9.38103  ],\n",
       "       [9.263828 ],\n",
       "       [9.4240055],\n",
       "       [9.419986 ],\n",
       "       [9.421608 ],\n",
       "       [9.167355 ],\n",
       "       [9.256482 ],\n",
       "       [9.182265 ],\n",
       "       [9.148032 ],\n",
       "       [9.44026  ],\n",
       "       [9.394941 ],\n",
       "       [9.2923565],\n",
       "       [9.112018 ],\n",
       "       [9.308989 ],\n",
       "       [9.322864 ],\n",
       "       [9.235062 ],\n",
       "       [9.362841 ],\n",
       "       [9.128745 ],\n",
       "       [9.229326 ],\n",
       "       [9.292768 ],\n",
       "       [9.245632 ],\n",
       "       [9.290768 ],\n",
       "       [9.387504 ],\n",
       "       [9.31956  ],\n",
       "       [9.423293 ],\n",
       "       [9.157164 ],\n",
       "       [9.396434 ],\n",
       "       [9.250191 ],\n",
       "       [9.388207 ],\n",
       "       [9.216769 ],\n",
       "       [9.291453 ],\n",
       "       [9.125174 ],\n",
       "       [9.329667 ],\n",
       "       [9.131066 ],\n",
       "       [9.377171 ],\n",
       "       [9.251799 ],\n",
       "       [9.280226 ],\n",
       "       [9.136898 ],\n",
       "       [9.137519 ],\n",
       "       [9.348102 ],\n",
       "       [9.352643 ],\n",
       "       [9.1946335],\n",
       "       [9.275095 ],\n",
       "       [9.168273 ],\n",
       "       [9.38303  ],\n",
       "       [9.338171 ],\n",
       "       [9.270222 ],\n",
       "       [9.263926 ],\n",
       "       [9.172569 ],\n",
       "       [9.380196 ],\n",
       "       [9.304453 ],\n",
       "       [9.24938  ],\n",
       "       [9.190424 ],\n",
       "       [9.25788  ],\n",
       "       [9.391141 ],\n",
       "       [9.164385 ],\n",
       "       [9.332533 ],\n",
       "       [9.249166 ],\n",
       "       [9.343913 ],\n",
       "       [9.373421 ],\n",
       "       [9.186588 ],\n",
       "       [9.397058 ],\n",
       "       [9.24559  ],\n",
       "       [9.191677 ],\n",
       "       [9.13451  ],\n",
       "       [9.394603 ],\n",
       "       [9.335138 ],\n",
       "       [9.11856  ],\n",
       "       [9.204791 ],\n",
       "       [9.22395  ],\n",
       "       [9.366761 ],\n",
       "       [9.4082775],\n",
       "       [9.165045 ],\n",
       "       [9.359676 ],\n",
       "       [9.226425 ],\n",
       "       [9.286492 ],\n",
       "       [9.2740135],\n",
       "       [9.292852 ],\n",
       "       [9.412077 ],\n",
       "       [9.132935 ],\n",
       "       [9.113696 ],\n",
       "       [9.226355 ],\n",
       "       [9.386689 ],\n",
       "       [9.185852 ],\n",
       "       [9.24346  ],\n",
       "       [9.36598  ],\n",
       "       [9.247137 ],\n",
       "       [9.409723 ],\n",
       "       [9.369325 ],\n",
       "       [9.148157 ],\n",
       "       [9.3275   ],\n",
       "       [9.429538 ],\n",
       "       [9.420509 ],\n",
       "       [9.20819  ],\n",
       "       [9.301236 ],\n",
       "       [9.195489 ],\n",
       "       [9.272644 ],\n",
       "       [9.173582 ],\n",
       "       [9.24122  ],\n",
       "       [9.123623 ],\n",
       "       [9.4035845],\n",
       "       [9.280516 ],\n",
       "       [9.377784 ],\n",
       "       [9.27151  ],\n",
       "       [9.420368 ],\n",
       "       [9.392583 ],\n",
       "       [9.398268 ],\n",
       "       [9.281038 ],\n",
       "       [9.116415 ],\n",
       "       [9.212271 ],\n",
       "       [9.4278555],\n",
       "       [9.393019 ],\n",
       "       [9.262981 ],\n",
       "       [9.120665 ],\n",
       "       [9.218046 ],\n",
       "       [9.12598  ],\n",
       "       [9.195226 ],\n",
       "       [9.402163 ],\n",
       "       [9.351032 ],\n",
       "       [9.173434 ],\n",
       "       [9.167263 ],\n",
       "       [9.268541 ],\n",
       "       [9.367141 ],\n",
       "       [9.286924 ],\n",
       "       [9.328594 ],\n",
       "       [9.246828 ],\n",
       "       [9.214007 ],\n",
       "       [9.191824 ],\n",
       "       [9.165275 ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = discriminator.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.save('./my_new_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(disc)\n",
    "new_discriminator = disc.Discriminator(quantile=quantile, loss_strategy=strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-26-af832292da6f>\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> 1 \u001b[0;31m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_discriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> b dadrah/selection/discriminator:102\n",
      "Breakpoint 2 at /eos/home-k/kiwoznia/dev/data_driven_anomaly_hunting/dadrah/selection/discriminator.py:102\n",
      "ipdb> new_discriminator.load('./my_new_model.h5')\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m<ipython-input-16-b579923c21eb>\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> 1 \u001b[0;31m\u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      2 \u001b[0;31m\u001b[0mnew_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./my_new_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n"
     ]
    }
   ],
   "source": [
    "import ipdb; ipdb.set_trace()\n",
    "new_discriminator.load('./my_new_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m<ipython-input-17-fb655b3cf6a1>\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> 1 \u001b[0;31m\u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      2 \u001b[0;31m\u001b[0mnew_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ipdb.set_trace()\n",
    "new_discriminator.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Discriminator' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a119eabab929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloaded_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Discriminator' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "loaded_weights = new_discriminator.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.52151966,  0.34217453,  0.01405535,  0.59535474,  0.69920963,\n",
       "        -0.1658983 , -0.44183353, -0.02300867, -0.06862972, -0.00430826,\n",
       "        -0.24678671,  0.7089294 ,  0.0510585 ,  0.01305069, -0.27448598,\n",
       "        -0.06475051, -0.4706956 , -0.4577948 ,  0.19247283, -0.01148568]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.52151966,  0.34217453,  0.01405535,  0.59535474,  0.69920963,\n",
       "        -0.1658983 , -0.44183353, -0.02300867, -0.06862972, -0.00430826,\n",
       "        -0.24678671,  0.7089294 ,  0.0510585 ,  0.01305069, -0.27448598,\n",
       "        -0.06475051, -0.4706956 , -0.4577948 ,  0.19247283, -0.01148568]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(weights)):\n",
    "    assert np.allclose(weights[0], loaded_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_loaded = loaded_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(y, y_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
